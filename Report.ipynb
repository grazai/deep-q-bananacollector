{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning to collect yellow bananas using Dueling Networks\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to train an agent using Deep Q-Learning and Dueling networks.\n",
    "### 1. Prepare \n",
    "\n",
    "Make sure, you followed the installation instructions which you can find in the [Readme](./Readme.md) of this repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#make sure, Environment is not going through proxy, when running on local machine\n",
    "os.environ['NO_PROXY'] = 'localhost,127.0.0.*'\n",
    "\n",
    "# define function for plotting scores\n",
    "def plot(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start environment\n",
    "\n",
    "Next, we will prepare the environment!  **_Before running the code cell below_**, change the `file_name` to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "Please enter the corresponding value before executing the next cell..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=r\"Banana/Banana\"\n",
    "train_mode = True  # Whether to run the environment in training or inference mode\n",
    "env_visible = True # Whether you want to see the agent doing training/inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... now we can actually start the environment.\n",
    "\n",
    "The UnityEnvironment launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain brains which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=file_name, base_port=64738, no_graphics=not env_visible)\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]\n",
    "brain_name = env.brain_names[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the state space\n",
    "\n",
    "We can reset the environment to be provided with an initial set of observations and states for the agent within the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "\n",
      "State size is: \n",
      "37\n",
      "\n",
      "Actions size is: \n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "print(\"\\nState size is: \\n{}\".format(state_size))\n",
    "action_size = brain.vector_action_space_size\n",
    "print(\"\\nActions size is: \\n{}\".format(action_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, we will use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "for _ in range(100):                               # maximum 200 steps\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        env.reset(train_mode=False)[brain_name]\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0, score: 3.0\n",
      "Run: 1, score: 7.0\n",
      "Run: 2, score: 7.0\n"
     ]
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "import torch\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint_vanilla.pth')) \n",
    "\n",
    "for i in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    for _ in range(100):\n",
    "        action = np.int32(agent.act(state))\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        state = env_info.vector_observations[0]        # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        score += reward\n",
    "        if done:\n",
    "            env.reset(train_mode=False)[brain_name]\n",
    "            break\n",
    "    print(\"Run: {}, score: {}\".format(i,score))\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your own agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deep Q-Learning Algorithm: Deep Q-Learning with experience replay\n",
    "\n",
    "So you want to train your own agent. Let's start with an introduction to the Deep Q-Learning algorithm. You can read more details [in this groundbreaking paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "From the [paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "\n",
    ">The basic idea behind reinforcement learning is to estimate the action-value function using the Bellman equation as an iterative update \\[...\\]. Such value iteration algorithms converge to the optimal action-value function \\[...\\]. In practice, this basic approach is impractical, because the action-value function is estimated separately for each sequence, without any generalization. Instead, it is common to use a function approximator to estimate the action-value function.\n",
    "\n",
    "In this implementation, you are using a neural network as function approximator. The weights of this neural network are updated at each learning step.\n",
    "\n",
    "The agent does not only learn from random actions, but also from experiences.\n",
    "After the agent takes a step in the environment, it's experiences (current state, action taken, reward, next state) at each timestep are stored in a dataset called experience buffer.\n",
    "\n",
    "This is the implementation of the DQN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from dqn_agent import Agent\n",
    "import torch\n",
    "\n",
    "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.005, eps_decay=0.995, train_mode=True):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode, \n",
    "                     in case the agent gets stuck in a state\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name] #initialize environment\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for _ in range(max_t):\n",
    "            action = np.int32(agent.act(state, eps))       # take an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            agent.step(state, action, reward, next_state, done) #save experience in replay buffer & update weights\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                env.reset(train_mode=train_mode)[brain_name]\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'my_checkpoint.pth')\n",
    "            break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the  agent \n",
    "\n",
    "We need to reset the environment and then reinitialize the agent. The environemtn is solved, when the agent reaches a score greater than or equal to 13.0 over the last 100 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3d3fb101833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "start=datetime.now()\n",
    "scores = dqn()\n",
    "end=datetime.now()\n",
    "print(\"Duration: {}\".format(end-start))\n",
    "plot(scores)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "\n",
    "The neural network used is a [duelling network](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "The dueling network architecture represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. According to the paper, the main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. The results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consists of two fully connected layers (512 and 128 neurons), followed by two heads called advantage (128 and 128 neurons) and value (128 and 1 neuron).\n",
    "The output is a q-value.\n",
    "\n",
    "\n",
    "\n",
    "from [dqn_agent.py](./dqn_agent.py)\n",
    "```python\n",
    "   # Q-Network\n",
    "   self.qnetwork_local = QNetwork(state_size, action_size, seed, 512, 128).to(device)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "from [model.py](./model.py)\n",
    "\n",
    "```python\n",
    "def __init__():\n",
    "  self.feature =nn.Sequential(\n",
    "                nn.Linear(state_size, fc1_units),\n",
    "                nn.SELU(), \n",
    "                nn.Linear(fc1_units, fc2_units),\n",
    "                nn.SELU() \n",
    "  )\n",
    "  \n",
    "  self.advantage = nn.Sequential(\n",
    "            nn.Linear(fc2_units, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, action_size)\n",
    "  )\n",
    "        \n",
    "  self.value = nn.Sequential(\n",
    "            nn.Linear(fc2_units, 128),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(128, 1)\n",
    "  )\n",
    "\n",
    "```\n",
    "\n",
    "This is how the two heads create one q-value from an advantage A(s,a) and a state value V(s).\n",
    "\n",
    "\n",
    "```python\n",
    "def forward(self, state):\n",
    "    x = self.feature(state)\n",
    "    advantage = self.advantage(x)\n",
    "    value     = self.value(x)\n",
    "    return value + advantage  - advantage.mean()\n",
    "```\n",
    "\n",
    "From the paper\n",
    "\n",
    "> The proposed network architecture, which we name the dueling architecture, <i>explicitly separates the representation of\n",
    "state values and (state-dependent) action advantages </i>. The\n",
    "dueling architecture consists of two streams that represent\n",
    "the value and advantage functions, while sharing a common convolutional feature learning module. The two streams\n",
    "are combined via a special aggregating layer to produce an\n",
    "estimate of the state-action value function Q \\[...\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further improvements\n",
    "\n",
    "As you can see, I played around with different activation functions in the neural network, ending up with [SELU](https://arxiv.org/abs/1706.02515), which sometimes helped, solving the environment quicker.\n",
    "Also using Huber loss improved performance on solving the environment.\n",
    "\n",
    "As next steps I intend to implement [Prioritized experience replay](https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. Experience transitions are uniformly sampled from a replay memory.\n",
    "\n",
    "However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. With prioritizing experience, important transitions are replayed more frequently, and therefore the agent can learn more efficiently.\n",
    "\n",
    "I'd also be interested in how [Noisy nets](https://arxiv.org/abs/1706.10295) could improve performance, as the authors claim that <i>the induced stochasticity of the agent's policy can be used to aid efficient exploration</i>. This can be of help, because the trained agent gets stuck a lot of times within 100 episodes or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd-pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
